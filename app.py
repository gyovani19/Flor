import gradio as gr
import torch
import cv2
import numpy as np
from matplotlib import pyplot as plt
from PIL import Image, ImageDraw
from transformers import AutoProcessor
from modeling_florence2 import Florence2ForConditionalGeneration
import io
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import Polygon  
import numpy as np
import random
import json


with open("config.json", "r") as f:
    config = json.load(f)

d_model = config['text_config']['d_model']
num_layers = config['text_config']['encoder_layers']
attention_heads = config['text_config']['encoder_attention_heads']
vocab_size = config['text_config']['vocab_size']
max_length = config['text_config']['max_length']
beam_size = config['text_config']['num_beams']
dropout = config['text_config']['dropout']
activation_function = config['text_config']['activation_function']
no_repeat_ngram_size = config['text_config']['no_repeat_ngram_size']
patch_size = config['vision_config']['patch_size'][0]
temporal_embeddings = config['vision_config']['visual_temporal_embedding']['max_temporal_embeddings']

title = """# ğŸ™‹ğŸ»â€â™‚ï¸Bem-vindo ao Ã“USI PREMIUM/florence"""
description = """
Este aplicativo apresenta o modelo **Ã“USI PREMIUM/florence**, um poderoso sistema de IA projetado para tarefas de **geraÃ§Ã£o de texto e imagem**. O modelo Ã© capaz de lidar com tarefas complexas como detecÃ§Ã£o de objetos, legendagem de imagens, OCR (Reconhecimento Ã“ptico de Caracteres) e anÃ¡lise detalhada de imagens baseadas em regiÃµes.

### Uso e Flexibilidade do Modelo

- **Sem RepetiÃ§Ã£o de N-Gramas**: Para reduzir a repetiÃ§Ã£o na geraÃ§Ã£o de texto, o modelo Ã© configurado com um **no_repeat_ngram_size** de **{no_repeat_ngram_size}**, garantindo saÃ­das mais diversificadas e significativas.
- **EstratÃ©gias de Amostragem**: Ã“USI PREMIUM/florence oferece estratÃ©gias de amostragem flexÃ­veis, incluindo **top-k** e **top-p (nucleus) sampling**, permitindo tanto geraÃ§Ã£o criativa quanto restrita, com base nas necessidades do usuÃ¡rio.

ğŸ“¸ğŸ“ˆâœğŸ»florence Ã© um modelo robusto capaz de lidar com vÃ¡rias tarefas de **texto e imagem** com alta precisÃ£o e flexibilidade, tornando-se uma ferramenta valiosa para pesquisas acadÃªmicas e aplicaÃ§Ãµes prÃ¡ticas.

### **Como Usar**:
1. **FaÃ§a o Upload de uma Imagem**: Selecione uma imagem para processamento.
2. **Escolha uma Tarefa**: Escolha uma tarefa no menu suspenso, como "Legenda", "DetecÃ§Ã£o de Objetos", "OCR", etc.
3. **Processar**: Clique no botÃ£o "Processar" para permitir que Ã“USI PREMIUM/florence analise a imagem e gere a saÃ­da.
4. **Ver Resultados**: Dependendo da tarefa, vocÃª verÃ¡ uma imagem processada (por exemplo, com caixas delimitadoras ou rÃ³tulos) ou um resultado baseado em texto (por exemplo, uma legenda gerada ou texto extraÃ­do).

VocÃª pode redefinir a interface a qualquer momento clicando no botÃ£o **Redefinir**.

### **Tarefas DisponÃ­veis**:
- **âœğŸ»Legenda**: Gere uma descriÃ§Ã£o concisa da imagem.
- **ğŸ“¸DetecÃ§Ã£o de Objetos**: Identifique e rotule objetos dentro da imagem.
- **ğŸ“¸âœğŸ»OCR**: Extraia texto da imagem.
- **ğŸ“¸Proposta de RegiÃ£o**: Detecte regiÃµes-chave na imagem para legendagem detalhada.
"""

model_presentation = f"""
O modelo **Ã“USI PREMIUM/florence** Ã© um modelo de ponta para tarefas de geraÃ§Ã£o condicional, projetado para ser altamente eficaz em tarefas de **texto** e **visÃ£o**. Ã‰ construÃ­do como uma arquitetura de **codificador-decodificador**, que permite maior flexibilidade e desempenho na geraÃ§Ã£o de saÃ­das com base em entradas diversificadas.

### Principais CaracterÃ­sticas

- **Arquitetura do Modelo**: Ã“USI PREMIUM/florence usa uma estrutura de codificador-decodificador, o que o torna eficaz em tarefas como **geraÃ§Ã£o de texto**, **resumo** e **traduÃ§Ã£o**. Ele possui **{num_layers} camadas** tanto para o codificador quanto para o decodificador, com uma dimensÃ£o do modelo (`d_model`) de **{d_model}**.
- **GeraÃ§Ã£o Condicional**: O modelo pode gerar texto condicionalmente, com um comprimento mÃ¡ximo de **{max_length} tokens** para cada sequÃªncia gerada, tornando-o ideal para tarefas que exigem saÃ­da concisa.
- **Busca em Feixe**: Ã“USI PREMIUM/florence suporta **busca em feixe** com atÃ© **{beam_size} feixes**, permitindo geraÃ§Ã£o de texto mais diversa e precisa explorando mÃºltiplas potenciais saÃ­das antes de selecionar a melhor.
- **TokenizaÃ§Ã£o**: Inclui um tokenizador com um vocabulÃ¡rio de **{vocab_size} tokens**. Tokens especiais como **bos_token_id (0)** e **eos_token_id (2)** ajudam a controlar o processo de geraÃ§Ã£o, marcando o inÃ­cio e o fim de uma sequÃªncia.
- **Mecanismo de AtenÃ§Ã£o**: Tanto o codificador quanto o decodificador utilizam **{attention_heads} cabeÃ§as de atenÃ§Ã£o** por camada, garantindo que o modelo possa focar em partes relevantes da entrada ao gerar texto.
- **Dropout e AtivaÃ§Ã£o**: Ã“USI PREMIUM/florence emprega uma **funÃ§Ã£o de ativaÃ§Ã£o {activation_function}** e uma **taxa de dropout de {dropout}**, o que melhora o desempenho do modelo prevenindo overfitting e melhorando a generalizaÃ§Ã£o.
- **ConfiguraÃ§Ã£o de Treinamento**: O modelo usa precisÃ£o **float32** para treinamento e suporta fine-tuning para tarefas especÃ­ficas ao configurar `finetuning_task` apropriadamente.

### IntegraÃ§Ã£o de VisÃ£o

AlÃ©m das tarefas de texto, Ã“USI PREMIUM/florence tambÃ©m incorpora **capacidades de visÃ£o**:
- **Processamento de Imagem Baseado em Patches**: O componente de visÃ£o opera em patches de imagem com um tamanho de patch de **{patch_size}x{patch_size}**.
- **Embedding Temporal**: Tarefas visuais se beneficiam de embeddings temporais com atÃ© **{temporal_embeddings} passos**, tornando o florence bem adequado para anÃ¡lise de vÃ­deo.
"""

joinus = """Ã“USI PREMIUM/florence Ã© um modelo de IA de ponta que oferece uma ampla gama de recursos para tarefas de texto e visÃ£o. Se vocÃª deseja colaborar, contribuir ou saber mais sobre o projeto, sinta-se Ã  vontade para entrar em contato conosco! Junte-se a nÃ³s para explorar o potencial da IA e criar soluÃ§Ãµes inovadoras para o futuro.
"""
how_to_use = """As configuraÃ§Ãµes avanÃ§adas permitem que vocÃª ajuste o processo de geraÃ§Ã£o de texto. Aqui estÃ¡ o que cada configuraÃ§Ã£o faz e como usÃ¡-la:

### Top-k (PadrÃ£o: 50)
A amostragem top-k limita a seleÃ§Ã£o do prÃ³ximo token aos k tokens mais provÃ¡veis.

- **Valores mais baixos** (por exemplo, 10) tornam a saÃ­da mais focada e determinÃ­stica.
- **Valores mais altos** (por exemplo, 100) permitem saÃ­das mais diversificadas.

**Exemplo:** Para uma tarefa de escrita criativa, tente definir top-k para 80 para uma linguagem mais variada.

### Top-p (PadrÃ£o: 1.0)
A amostragem top-p (ou nucleus) seleciona do menor conjunto de tokens cuja probabilidade cumulativa excede p.

- **Valores mais baixos** (por exemplo, 0.5) tornam a saÃ­da mais focada e coerente.
- **Valores mais altos** (por exemplo, 0.9) permitem saÃ­das mais diversificadas e potencialmente criativas.

**Exemplo:** Para uma legenda factual, defina top-p para 0.7 para equilibrar precisÃ£o e criatividade.

### Penalidade de RepetiÃ§Ã£o (PadrÃ£o: 1.0)
Esta penaliza a repetiÃ§Ã£o no texto gerado.

- **Valores prÃ³ximos a 1.0** tÃªm efeito mÃ­nimo na repetiÃ§Ã£o.
- **Valores mais altos** (por exemplo, 1.5) desencorajam mais fortemente a repetiÃ§Ã£o.

**Exemplo:** Se vocÃª notar frases repetidas, tente aumentar para 1.2 para um texto mais variado.

### NÃºmero de Feixes (PadrÃ£o: 3)
A busca em feixe explora mÃºltiplas sequÃªncias possÃ­veis em paralelo.

- **Valores mais altos** (por exemplo, 5) podem levar a melhor qualidade, mas geraÃ§Ã£o mais lenta.
- **Valores mais baixos** (por exemplo, 1) sÃ£o mais rÃ¡pidos, mas podem produzir resultados de menor qualidade.

**Exemplo:** Para tarefas complexas como legendagem densa, tente aumentar para 5 feixes.

### MÃ¡ximo de Tokens (PadrÃ£o: 512)
Define o comprimento mÃ¡ximo do texto gerado.

- **Valores mais baixos** (por exemplo, 100) para saÃ­das concisas.
- **Valores mais altos** (por exemplo, 1000) para descriÃ§Ãµes mais detalhadas.

**Exemplo:** Para uma descriÃ§Ã£o detalhada da imagem, defina o mÃ¡ximo de tokens para 800 para uma saÃ­da abrangente.

Lembre-se, essas configuraÃ§Ãµes interagem entre si, entÃ£o experimentar diferentes combinaÃ§Ãµes pode levar a resultados interessantes!
"""
device = "cuda" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

model = Florence2ForConditionalGeneration.from_pretrained("PleIAs/Florence-PDF", torch_dtype=torch_dtype, trust_remote_code=True).to(device)
processor = AutoProcessor.from_pretrained("PleIAs/Florence-PDF", trust_remote_code=True)

TASK_PROMPTS = {
    "âœğŸ»Caption": "<CAPTION>",
    "âœğŸ»âœğŸ»Caption": "<DETAILED_CAPTION>",
    "âœğŸ»âœğŸ»âœğŸ»Caption": "<MORE_DETAILED_CAPTION>",
    "ğŸ“¸Object Detection": "<OD>",
    "ğŸ“¸Dense Region Caption": "<DENSE_REGION_CAPTION>",
    "ğŸ“¸âœğŸ»OCR": "<OCR>",
    "ğŸ“¸âœğŸ»OCR with Region": "<OCR_WITH_REGION>",
    "ğŸ“¸Region Proposal": "<REGION_PROPOSAL>",
    "ğŸ“¸âœğŸ»Object Detection with Description": "<OD>",  # Start with Object Detection
    # We will handle the detailed description separately in the code
}

# Update IMAGE_TASKS and TEXT_TASKS
IMAGE_TASKS = ["ğŸ“¸Object Detection", "ğŸ“¸Dense Region Caption", "ğŸ“¸Region Proposal", "ğŸ“¸âœğŸ»OCR with Region", "ğŸ“¸âœğŸ»Object Detection with Description"]
TEXT_TASKS = ["âœğŸ»Caption", "âœğŸ»âœğŸ»Caption", "âœğŸ»âœğŸ»âœğŸ»Caption", "ğŸ“¸âœğŸ»OCR", "ğŸ“¸âœğŸ»OCR with Region", "ğŸ“¸âœğŸ»Object Detection with Description"]

colormap = ['blue','orange','green','purple','brown','pink','gray','olive','cyan','red',
            'lime','indigo','violet','aqua','magenta','coral','gold','tan','skyblue']

def fig_to_pil(fig):
    buf = io.BytesIO()
    fig.savefig(buf, format='png')
    buf.seek(0)
    return Image.open(buf)

def plot_bbox(image, data, use_quad_boxes=False):
    fig, ax = plt.subplots()
    ax.imshow(image)

    if use_quad_boxes:
        for quad_box, label in zip(data.get('quad_boxes', []), data.get('labels', [])):
            quad_box = np.array(quad_box).reshape(-1, 2)
            poly = Polygon(quad_box, linewidth=1, edgecolor='r', facecolor='none')
            ax.add_patch(poly)
            plt.text(quad_box[0][0], quad_box[0][1], label, color='white', fontsize=8,
                     bbox=dict(facecolor='red', alpha=0.5))
    else:
        bboxes = data.get('bboxes', [])
        labels = data.get('labels', [])
        for bbox, label in zip(bboxes, labels):
            x1, y1, x2, y2 = bbox
            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor='r', facecolor='none')
            ax.add_patch(rect)
            plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))

    ax.axis('off')

    return fig

def draw_ocr_bboxes(image, prediction):
    scale = 1
    draw = ImageDraw.Draw(image)
    bboxes, labels = prediction['quad_boxes'], prediction['labels']
    for box, label in zip(bboxes, labels):
        color = random.choice(colormap)
        new_box = (np.array(box) * scale).tolist()
        draw.polygon(new_box, width=3, outline=color)
        draw.text((new_box[0]+8, new_box[1]+2),
                  "{}".format(label),
                  align="right",
                  fill=color)
        
    return image

def draw_bounding_boxes(image, quad_boxes, labels, color=(0, 255, 0), thickness=2):
    """
    Draws quadrilateral bounding boxes on the image.
    """
    for i, quad in enumerate(quad_boxes):
        points = np.array(quad, dtype=np.int32).reshape((-1, 1, 2))  # Reshape the quad points for drawing
        image = cv2.polylines(image, [points], isClosed=True, color=color, thickness=thickness)
        label_pos = (int(quad[0]), int(quad[1]) - 10)  
        cv2.putText(image, labels[i], label_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness)

    return image

def process_image(image, task):
    prompt = TASK_PROMPTS[task]
    inputs = processor(text=prompt, images=image, return_tensors="pt").to(device, torch_dtype)
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=1024,
        num_beams=3,
        do_sample=False
    )

    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
    parsed_answer = processor.post_process_generation(generated_text, task=prompt, image_size=(image.width, image.height))

    return parsed_answer


def main_process(image, task, top_k, top_p, repetition_penalty, num_beams, max_tokens):
    prompt = TASK_PROMPTS[task]
    inputs = processor(text=prompt, images=image, return_tensors="pt").to(device, torch_dtype)
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=max_tokens,
        num_beams=num_beams,
        do_sample=True,
        top_k=top_k,
        top_p=top_p,
        repetition_penalty=repetition_penalty
    )

    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
    parsed_answer = processor.post_process_generation(generated_text, task=prompt, image_size=(image.width, image.height))
    return parsed_answer

def process_and_update(image, task, top_k, top_p, repetition_penalty, num_beams, max_tokens):
    if image is None:
        return None, gr.update(visible=False), "Please upload an image first.", gr.update(visible=True)
    
    if task == "ğŸ“¸âœğŸ»Object Detection with Description":
        # Perform Object Detection first
        od_prompt = TASK_PROMPTS["ğŸ“¸Object Detection"]
        od_inputs = processor(text=od_prompt, images=image, return_tensors="pt").to(device, torch_dtype)
        od_generated_ids = model.generate(
            **od_inputs,
            max_new_tokens=max_tokens,
            num_beams=num_beams,
            do_sample=True,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty
        )
        od_generated_text = processor.batch_decode(od_generated_ids, skip_special_tokens=False)[0]
        od_parsed_answer = processor.post_process_generation(od_generated_text, task=od_prompt, image_size=(image.width, image.height))
        
        # Display Bounding Boxes
        fig = plot_bbox(image, od_parsed_answer.get('<OD>', {}))
        output_image = fig_to_pil(fig)
        
        # Then perform Detailed Description
        dd_prompt = TASK_PROMPTS["âœğŸ»âœğŸ»âœğŸ»Caption"]
        dd_inputs = processor(text=dd_prompt, images=image, return_tensors="pt").to(device, torch_dtype)
        dd_generated_ids = model.generate(
            **dd_inputs,
            max_new_tokens=max_tokens,
            num_beams=num_beams,
            do_sample=True,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty
        )
        dd_generated_text = processor.batch_decode(dd_generated_ids, skip_special_tokens=False)[0]
        dd_parsed_answer = processor.post_process_generation(dd_generated_text, task=dd_prompt, image_size=(image.width, image.height))
        text_output = str(dd_parsed_answer)
        
        return output_image, gr.update(visible=True), text_output, gr.update(visible=True)
    else:
        # Existing processing for other tasks
        result = main_process(image, task, top_k, top_p, repetition_penalty, num_beams, max_tokens)
        
        if task in IMAGE_TASKS:
            if task == "ğŸ“¸âœğŸ»OCR with Region":
                fig = plot_bbox(image, result.get('<OCR_WITH_REGION>', {}), use_quad_boxes=True)
                output_image = fig_to_pil(fig)
                text_output = result.get('<OCR_WITH_REGION>', {}).get('recognized_text', 'No text found')
                return output_image, gr.update(visible=True), text_output, gr.update(visible=True)
            else:
                fig = plot_bbox(image, result.get(TASK_PROMPTS[task], {}))
                output_image = fig_to_pil(fig)
                return output_image, gr.update(visible=True), None, gr.update(visible=False)
        else:
            return None, gr.update(visible=False), str(result), gr.update(visible=True)

def reset_outputs():
    return None, gr.update(visible=False), None, gr.update(visible=True)

with gr.Blocks(title="Tonic's ğŸ™ğŸ»PLeIAs/ğŸ“¸ğŸ“ˆâœğŸ»Florence-PDF") as iface:
    with gr.Column():
        with gr.Row():
            gr.Markdown(title)
        with gr.Row():
            with gr.Column(scale=1):
                with gr.Group():                    
                    gr.Markdown(model_presentation)
            with gr.Column(scale=1):
                with gr.Group():
                    gr.Markdown(description)
        with gr.Row():
            with gr.Accordion("ğŸ«±ğŸ»â€ğŸ«²ğŸ»Join Us", open=True):
                gr.Markdown(joinus)
        with gr.Row():
            with gr.Column(scale=1):
                image_input = gr.Image(type="pil", label="Input Image")
                task_dropdown = gr.Dropdown(list(TASK_PROMPTS.keys()), label="Task", value="âœğŸ»Caption")            
                with gr.Row():
                    submit_button = gr.Button("ğŸ“¸ğŸ“ˆâœğŸ»Process")
                    reset_button = gr.Button("â™»ï¸Reset")
                with gr.Accordion("ğŸ§ªAdvanced Settings", open=False):
                    with gr.Accordion("ğŸ—ï¸How To Use", open=True):
                        gr.Markdown(how_to_use)                        
                    top_k = gr.Slider(minimum=1, maximum=100, value=50, step=1, label="Top-k")
                    top_p = gr.Slider(minimum=0.0, maximum=1.0, value=1.0, step=0.01, label="Top-p")
                    repetition_penalty = gr.Slider(minimum=1.0, maximum=2.0, value=1.0, step=0.01, label="Repetition Penalty")
                    num_beams = gr.Slider(minimum=1, maximum=6, value=3, step=1, label="Number of Beams")
                    max_tokens = gr.Slider(minimum=1, maximum=1024, value=1000, step=1, label="Max Tokens")
            with gr.Column(scale=1):    
                output_image = gr.Image(label="Ã“USI PREMIUM/florence", visible=False)
                output_text = gr.Textbox(label="Ã“USI PREMIUM/florence", visible=False)
    
    submit_button.click(
        fn=process_and_update,
        inputs=[image_input, task_dropdown, top_k, top_p, repetition_penalty, num_beams, max_tokens],
        outputs=[output_image, output_image, output_text, output_text]
    )
    
    reset_button.click(
        fn=reset_outputs,
        inputs=[],
        outputs=[output_image, output_image, output_text, output_text]
    )
    
    task_dropdown.change(
        fn=lambda task: (gr.update(visible=task in IMAGE_TASKS), gr.update(visible=task in TEXT_TASKS)),
        inputs=[task_dropdown],
        outputs=[output_image, output_text]
    )

iface.launch()